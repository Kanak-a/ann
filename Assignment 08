import numpy as np

# Define the sigmoid function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Define the derivative of the sigmoid function
def sigmoid_derivative(x):
    return x * (1 - x)

# Define the number of inputs, hidden units, and outputs
n_inputs = 2
n_hidden = 3
n_outputs = 1

# Initialize the weights and biases
weights1 = np.random.rand(n_inputs, n_hidden)
weights2 = np.random.rand(n_hidden, n_outputs)
bias1 = np.random.rand(1, n_hidden)
bias2 = np.random.rand(1, n_outputs)

# Define the training data
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [0], [0], [1]])

# Define the learning rate
learning_rate = 0.1

# Train the network
for i in range(1000):
    # Forward pass
    hidden_layer = sigmoid(np.dot(X, weights1) + bias1)
    output_layer = sigmoid(np.dot(hidden_layer, weights2) + bias2)

    # Backward pass
    output_error = y - output_layer
    hidden_error = output_error.dot(weights2.T) * sigmoid_derivative(hidden_layer)

    # Update the weights and biases
    weights2 += learning_rate * hidden_layer.T.dot(output_error)
    bias2 += learning_rate * np.sum(output_error, axis=0, keepdims=True)
    weights1 += learning_rate * X.T.dot(hidden_error)
    bias1 += learning_rate * np.sum(hidden_error, axis=0, keepdims=True)

# Test the network
hidden_layer = sigmoid(np.dot(X, weights1) + bias1)
output_layer = sigmoid(np.dot(hidden_layer, weights2) + bias2)
print("Output:")
print(output_layer)
